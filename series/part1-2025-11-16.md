# Why Your LLM Can't Be Your Reporting Layer

*Part 1 of "Scaling LLM-Based Reporting Past the Demo"*

So you've hooked an LLM up to your database and asked it to "show me all customers in California." Works great in the demo - clean results, natural language interface, everyone's impressed. Then you try it with real data and watch your context window explode, your API costs spike, and your responses slow to a crawl.

Welcome to the wall that every LLM-based reporting project hits.

## The Fix, in 30 Seconds

Before we dig into why this happens, here's the solution: **don't make the LLM responsible for delivering results - just for generating the query.**

Every query tool returns two things:

1. A **sample** (10-50 rows) - enough for the LLM to validate its work
2. A **resource link** - a handle the app uses to fetch the full dataset

The LLM sees the sample, confirms the query looks right, and tells the user "Found 847 institutions matching your criteria." Your app fetches all 847 rows through the resource link and renders a proper table. The LLM never touches those 847 rows.

Here's what's interesting: if you're building with the Model Context Protocol, the mechanism for this already exists. MCP's `resource_link` content type - introduced in the 2025-06-18 spec - lets tools return URI references that clients can fetch out-of-band. It's designed exactly for this scenario, but I've seen remarkably few implementations actually use it for reporting. Most developers either don't know it exists or aren't sure how to structure their tools around it.

That's the gap this series fills. We're not inventing new protocol features - we're using MCP's existing primitives in a coordinated pattern optimized for large dataset reporting.

The rest of this post explains why this pattern is necessary - and why simpler approaches fail.

## Why LLMs Are So Attractive Here

Consider a dataset like IPEDS - the federal database of U.S. colleges and universities that we'll use as a running example. You've got thousands of institutions, each with hundreds of attributes: state, control type (public/private), Carnegie classification, program offerings, enrollment figures across a decade. Users want to search by any combination - "private research universities in the Northeast with growing computer science programs."

Traditional UI approaches force a bad choice. Make it simple with a few dropdowns, and power users hit walls constantly. Expose everything, and you've got 40 controls that intimidate most users while still missing edge cases.

SQL is genuinely the right tool for this kind of query. But your users aren't programmers. We've been chasing natural language query for decades because of this gap.

LLMs change the equation. They're a real-time programmer on call - translating "show me nursing programs at public universities in California" into the exact query needed. No rigid UI, no SQL knowledge required. 

*That's* why this is exciting. And that's why it's worth solving the problems that come next.

## Two Different Jobs

Here's the distinction that matters: LLMs do two fundamentally different things with data, and only one of them breaks at scale.

**Analysis tasks** need the LLM to compute an answer. "What's the average CS enrollment at Ivy League schools?" - that's 8 schools, maybe 5 columns, 40 data points. The LLM needs to see this data to do the math. Fine.

**Retrieval tasks** need the LLM to generate a query, but the *user* needs the results. "Show me all institutions with computer science programs" - that's 2,400+ schools. The LLM's job is translating "computer science" into CIP code 11.0701 and building the query. It doesn't need 2,400 rows to do that.

Most reporting requests are retrieval. And retrieval is where context windows become a problem.

## The Math That Kills Your Demo

Let's get concrete. IPEDS - the Integrated Postsecondary Education Data System - is the federal database that every U.S. college reports to annually. It's the canonical source for institutional data: enrollments, completions, finances, admissions, you name it.

- ~6,000 institutions
- ~2,000 CIP codes (the Classification of Instructional Programs - the taxonomy for academic programs, where 11.0701 is "Computer Science" and 51.3801 is "Registered Nursing")
- 10+ years of completion data
- Each institution has 100+ metadata fields

This isn't obscure data. Enrollment management teams use it to benchmark against peer institutions. Marketing needs competitive analysis for program launches. Academic administrators track trends to justify new programs or sunset declining ones. Accreditation reviews demand it. Everyone needs slightly different slices, filtered and grouped in ways no fixed dashboard anticipates.

"Show me nursing program enrollment trends across public universities in the Southeast" could return 50,000+ rows. Each row might be 50-100 tokens with field names and values. You're looking at millions of tokens for a single query result.

Context windows are finite. Even with 128K or 200K token models, you're burning through budget fast. Worse, attention mechanisms have quadratic complexity - costs scale with the square of context length. And research shows LLMs exhibit "lost in the middle" degradation: accuracy drops for information buried in long contexts.

You're paying more for worse results.

## The Hidden Cost: Query Iteration

Here's something that makes this even worse: LLMs don't get queries right on the first try.

Studies show 15-20% error rates even on well-structured text-to-SQL benchmarks. In practice, LLMs need to *see* output to catch mistakes - wrong joins, missing filters, unexpected nulls. An LLM might generate a query for "computer science programs" that accidentally includes certificate programs when you wanted bachelor's degrees. It needs to see sample output to catch that.

This means iteration. The LLM generates a query, looks at results, refines, tries again. If you're cramming full result sets into context on every attempt, you're paying for 50,000 rows of garbage before you even get to the correct query.

The sample in dual response solves this. The LLM gets 10-15 representative rows - enough to validate columns, spot data issues, and refine. When the query is finally correct, the full results flow through the resource link, never touching the context window.

## What Doesn't Work

Before I landed on dual response, I tried the obvious alternatives:

**"Just paginate."** The LLM still needs to see *something* to validate the query. And who's driving the pagination? The LLM? Now you're doing multiple round-trips, each consuming context.

**"Summarize first."** Great, now you've lost the detail users actually want. They asked for a list of institutions, not a statistical summary.

**"Use RAG."** Wrong tool. RAG is for finding relevant documents in unstructured text. This is structured query generation - completely different problem.

**"Bigger context windows."** Every developer's first instinct. But cost scales with context, latency tanks, and accuracy degrades. You're not solving the problem, just postponing it to a larger dataset.

## Two Flavors of Tools

One more thing before we tie this together. There are two ways to expose data to an LLM:

**API-style tools** have structured parameters:
```typescript
search_institutions({ state: "NJ", control: "public" })
```

**SQL-style tools** let the LLM write queries directly:
```typescript
query("SELECT * FROM institutions WHERE state = 'NJ'")
```

API-style is safer (no injection risk) and easier for LLMs to get right. SQL-style is more flexible but needs query validation. Either way, both can return thousands of rows - and both need dual response.

## Why Dual Response Works

Let's make this concrete. Here's what your query tool returns - structured to align with MCP's tool response format:

```typescript
{
  results: [...],        // 10-15 sample rows
  resource: {
    uri: "resource://abc123",
    name: "Institution Search Results",
    mimeType: "application/json"
  },
  metadata: {
    total_count: 847,
    columns: ["name", "state", "enrollment", ...],
    expires_at: "2025-01-15T10:30:00Z"
  }
}
```

The `resource` object maps directly to MCP's `resource_link` content type. Your tool returns this alongside the sample data, and the host application knows it can fetch the full dataset via that URI without involving the LLM.

The LLM sees 10-15 rows - maybe 500-1,000 tokens - plus metadata telling it the full result set has 847 records. That's enough to validate the query is correct: right columns, expected data types, reasonable values. The LLM responds to the user: "Found 847 institutions matching your criteria. [View Results]"

Your app takes that resource URI, calls a REST endpoint, and fetches all 847 rows with pagination. Renders a sortable table, generates a CSV export, builds a chart - whatever the user needs. The LLM is already done.

**The token math matters.** Those 847 full rows might be 40,000+ tokens. With dual response, the LLM sees ~1,000. That's a 40x reduction on a single query. Factor in iteration - the LLM typically refines 2-3 times before getting the query right - and you're avoiding 80,000-120,000 tokens of wasted context on intermediate attempts.

**Speed follows from tokens.** LLM latency scales with context length. A response drawing on 1,000 tokens of tool output comes back in a second or two. One drawing on 40,000 tokens? You're waiting. For iterative refinement, this compounds - each round-trip is slower when you're hauling full result sets.

**Cost is obvious but worth stating.** API pricing is per-token. 40x fewer tokens in tool responses means dramatically lower costs per query. For a reporting tool with real usage, this is the difference between viable and not.

Now you can see why the pattern solves each problem:

- **Context limits?** The LLM only sees the sample.
- **Query iteration?** The sample gives enough to validate without paying for full results on every attempt.
- **Users need full data?** The resource link lets your app fetch and render everything outside the LLM's context.
- **Both tool styles?** The server returns the same dual response structure regardless of how the query was specified.

The LLM's job is understanding intent and generating queries. The app's job is rendering results. Dual response enforces this separation cleanly.

---

In [Part 2](/link), I'll show how this pattern maps to MCP's actual primitives - `resource_link`, `structuredContent`, `outputSchema` - and how to coordinate them into a complete reporting architecture. We'll cover the REST layer for out-of-band retrieval, resource lifecycle management (those speculative queries pile up fast), and the prompt engineering that makes LLMs work with this structure reliably.

If you're already familiar with MCP, you might be surprised how much of the infrastructure is already there. The protocol gives you the building blocks; what's been missing is a documented pattern for combining them.

I've written up the formal specification in a [paper](https://arxiv.org/abs/2510.05968) presented at ACDSA 2026 if you want the academic details. If you want to skip ahead to code, check out the [working implementation](https://github.com/freezer333/mcp-rlp).

---

*Next: [Part 2 - The Dual Response Pattern](/link)*
